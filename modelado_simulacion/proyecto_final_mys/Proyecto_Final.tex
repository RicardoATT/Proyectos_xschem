\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\setlength{\parskip}{12pt}
\usepackage[numbers]{natbib}
%\renewcommand{\refname}{Referencias}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Integraci√≥n de memristores en redes neuronales para la clasificaci√≥n del dataset MNIST utilizando TensorFlow\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Ricardo Aldair Tirado Torres}
\IEEEauthorblockA{\textit{Centro de Investigaci√≥n en Computaci√≥n} \\
\textit{Instituto Polit√©cnico Nacional}\\
Ciudad de M√©xico, M√©xico \\
rtiradot2023@cic.ipn.mx}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Ricardo Barr√≥n Fern√°ndez}
\IEEEauthorblockA{\textit{Centro de Investigaci√≥n en Computaci√≥n} \\
	\textit{Instituto Polit√©cnico Nacional}\\
	Ciudad de M√©xico, M√©xico \\
	rbarron@cic.ipn.mx}
}
\maketitle

\begin{abstract}
El c√≥mputo neurom√≥rfico, con sus redes neuronales pulsantes, el aprendizaje STDP y los memristores, representa un paradigma revolucionario en la inform√°tica. La integraci√≥n de estos conceptos en la tecnolog√≠a CMOS no solo promete un salto significativo en la eficiencia y capacidad de las m√°quinas, sino que tambi√©n nos acerca un paso m√°s a replicar la asombrosa complejidad y eficiencia del cerebro humano. Esta tecnolog√≠a abre un abanico de posibilidades en el campo de la inteligencia artificial y m√°s all√°, con el potencial de transformar la forma en que interactuamos con las m√°quinas y el mundo que nos rodea. En este trabajo se utiliza al nodo tecnol√≥gico de SKY130, un proceso de fabricaci√≥n que integra los elementos necesarios para dise√±ar una red neuronal pulsante que integra el concepto del aprendizaje STDP no supervisado.
\end{abstract}

\begin{IEEEkeywords}
Redes neuronales artificiales, sinapsis memristiva, clasificaci√≥n de patrones, memristor HP.
\end{IEEEkeywords}

\section{Introducci√≥n}
Las redes neuronales artificiales (ANNs, por sus siglas en ingl√©s) son modelos computacionales inspirados en la estructura y el funcionamiento del cerebro humano, dise√±ados para realizar tareas de reconocimiento de patrones, clasificaci√≥n y predicci√≥n. Estas redes consisten en capas de neuronas artificiales conectadas entre s√≠, donde cada conexi√≥n tiene un peso asociado que se ajusta durante el proceso de entrenamiento. El entrenamiento de una ANN implica la optimizaci√≥n de estos pesos mediante algoritmos de aprendizaje, siendo el descenso de gradiente uno de los m√©todos m√°s comunes. Una vez entrenada, la red puede realizar inferencias sobre datos nuevos, aplicando los pesos optimizados para generar predicciones o clasificaciones.

En el √°mbito de la inteligencia artificial, las ANNs se utilizan extensamente para la clasificaci√≥n de patrones, un proceso crucial en aplicaciones como el reconocimiento de im√°genes, el procesamiento del lenguaje natural y la detecci√≥n de fraudes. El aprendizaje profundo, una subrama del aprendizaje autom√°tico que se basa en ANNs con m√∫ltiples capas (redes neuronales profundas), ha revolucionado estos campos, logrando avances significativos en precisi√≥n y eficiencia.

Los memristores han emergido como una tecnolog√≠a prometedora para implementar pesos sin√°pticos en redes neuronales artificiales. Un memristor es un componente electr√≥nico cuya resistencia puede ser ajustada y memorizada, comport√°ndose de manera an√°loga a una sinapsis biol√≥gica. En el contexto del aprendizaje m√°quina y el aprendizaje profundo, los memristores ofrecen una soluci√≥n eficiente para el almacenamiento y ajuste de pesos sin√°pticos, permitiendo la creaci√≥n de hardware neurom√≥rfico que puede ejecutar tareas de inferencia y entrenamiento de manera m√°s r√°pida y con menor consumo de energ√≠a en comparaci√≥n con los sistemas tradicionales basados en transistores.

El memristor de HP, desarrollado por Hewlett-Packard, es uno de los primeros y m√°s conocidos dispositivos de este tipo, capaz de recordar su estado resistivo incluso despu√©s de apagar la energ√≠a. Este atributo lo hace ideal para su uso en ANNs, donde puede representar y ajustar eficientemente los pesos sin√°pticos, facilitando as√≠ la implementaci√≥n de redes neuronales en dispositivos de hardware especializados.

Para evaluar y entrenar redes neuronales, se utilizan conjuntos de datos estandarizados que permiten medir el rendimiento y la precisi√≥n del modelo. Uno de los conjuntos de datos m√°s utilizados en la investigaci√≥n de ANNs es el MNIST (Modified National Institute of Standards and Technology), que contiene 60,000 im√°genes de entrenamiento y 10,000 im√°genes de prueba de d√≠gitos escritos a mano, cada una con una resoluci√≥n de 28x28 p√≠xeles. Este conjunto de datos ha sido fundamental para el desarrollo y benchmarking de nuevas arquitecturas de redes neuronales y t√©cnicas de aprendizaje autom√°tico.

Este reporte explorar√° en detalle los conceptos mencionados, comenzando con una revisi√≥n de las redes neuronales artificiales, seguido de una discusi√≥n sobre el papel de los memristores como pesos sin√°pticos, destacando el memristor de HP, y finalizando con un an√°lisis del conjunto de datos MNIST y su relevancia en el campo de la inteligencia artificial, el aprendizaje m√°quina y el aprendizaje profundo.

A lo largo de este trabajo, discutimos la teor√≠a subyacente de los memristores, detallamos la arquitectura de nuestra red neuronal y evaluamos el desempe√±o de nuestro modelo en la tarea de clasificaci√≥n de im√°genes. Los resultados obtenidos muestran que la integraci√≥n de memristores en redes neuronales es una estrategia prometedora que puede contribuir significativamente al desarrollo de sistemas de IA m√°s avanzados y eficientes.

\section{Preliminares}

\subsection{Redes neuronales artificiales}
Las redes neuronales artificiales son modelos computacionales inspirados en la estructura y el funcionamiento del cerebro humano. Estas redes est√°n compuestas por unidades interconectadas llamadas neuronas artificiales, distribuidas t√≠picamente en tres tipos de capas:

\begin{itemize}
	\item \textit{Capa de entrada}: Recibe las se√±ales iniciales del entorno externo. En el caso de la clasificaci√≥n de im√°genes, cada neurona de la capa de entrada representa un p√≠xel de la imagen.
	\item \textit{Capas ocultas}: Procesan las se√±ales de entrada mediante una serie de transformaciones no lineales. Estas capas est√°n compuestas por neuronas que aplican funciones de activaci√≥n a sus entradas ponderadas.
	\item \textit{Capa de salida}: Genera la salida final del modelo. Para la clasificaci√≥n, cada neurona en la capa de salida puede representar una clase diferente.
\end{itemize}

Cada neurona en una red neuronal artificial realiza una operaci√≥n matem√°tica sobre sus entradas ponderadas y una funci√≥n de activaci√≥n. La salida de una neurona $j$ se puede representar matem√°ticamente como:

\begin{equation}
	y_{j} = \sigma \left( \sum_{i} w_{ij}x_{i} + b_{i} \right)
	\label{eq:1}
\end{equation}

Donde:

\begin{itemize}
	\item $y_{j}$ es la salida de la neurona .
	\item $\sigma$ es la funci√≥n de activaci√≥n (por ejemplo, ReLU, sigmoide, tanh).
	\item $w_{ij}$ es el peso sin√°ptico entre la neurona $i$ de la capa anterior y la neurona $j$.
	\item $x_{i}$ es la entrada desde la neurona $i$ de la capa anterior.
	\item $b_{j}$ es el sesgo de la neurona $j$
\end{itemize}

El proceso de entrenamiento de una red neuronal implica ajustar los pesos sin√°pticos $w_{ij}$ y los sesgos $b_{j}$ para minimizar una funci√≥n de p√©rdida, que mide la diferencia entre las predicciones de la red y los valores reales. Este ajuste se realiza mediante un algoritmo de optimizaci√≥n, como el gradiente descendente, junto con un proceso de retropropagaci√≥n del error.

Propagaci√≥n hacia Adelante: Los datos de entrada se propagan a trav√©s de la red para generar una predicci√≥n.
C√°lculo de la P√©rdida: Se calcula la funci√≥n de p√©rdida que mide la discrepancia entre la predicci√≥n y la etiqueta real.
Retropropagaci√≥n: El error se propaga hacia atr√°s a trav√©s de la red para calcular los gradientes de la funci√≥n de p√©rdida respecto a los pesos y sesgos.
Actualizaci√≥n de Pesos: Los pesos y sesgos se actualizan utilizando el algoritmo de optimizaci√≥n.

Propagaci√≥n hacia Adelante:

Las entradas se pasan a trav√©s de la capa de entrada.
Las salidas de cada neurona de la capa de entrada se multiplican por los pesos correspondientes y se pasan a las neuronas de la primera capa oculta.
Cada neurona de la capa oculta suma sus entradas ponderadas y aplica una funci√≥n de activaci√≥n (como ReLU, sigmoide o tanh) para calcular su salida.
Este proceso se repite para todas las capas ocultas hasta llegar a la capa de salida.
La capa de salida produce la predicci√≥n final del modelo.
C√°lculo de la P√©rdida:

La salida de la red se compara con la etiqueta verdadera utilizando una funci√≥n de p√©rdida (como la entrop√≠a cruzada esparsa para problemas de clasificaci√≥n).
La funci√≥n de p√©rdida cuantifica la discrepancia entre la predicci√≥n del modelo y el valor verdadero.
Retropropagaci√≥n del Error:

El error calculado se propaga hacia atr√°s a trav√©s de la red.
Se calculan los gradientes de la funci√≥n de p√©rdida respecto a los pesos y sesgos utilizando el algoritmo de retropropagaci√≥n.
Estos gradientes indican la direcci√≥n y magnitud en la que deben ajustarse los pesos para reducir la p√©rdida.
Actualizaci√≥n de Pesos:

Los pesos y sesgos se actualizan utilizando un algoritmo de optimizaci√≥n, como el gradiente descendente o una de sus variantes (por ejemplo, Adam, RMSprop).
La actualizaci√≥n de los pesos se realiza para minimizar la funci√≥n de p√©rdida y mejorar la precisi√≥n del modelo.

Uno de los tipos m√°s simples y ampliamente utilizados de redes neuronales artificiales son las redes neuronales \textit{feedforward} (FFNN). En una FFNN, las conexiones entre las unidades no forman ciclos, y la informaci√≥n fluye solo en una direcci√≥n: desde las capas de entrada, a trav√©s de las capas ocultas, y finalmente hacia la capa de salida.

Existen varios tipos de redes neuronales artificiales

\subsection{Memristores}
Los memristores (resistores de memoria) son componentes el√©ctricos pasivos que han sido teorizados durante d√©cadas y que finalmente se desarrollaron experimentalmente en los √∫ltimos a√±os. Estos dispositivos se caracterizan por su capacidad para recordar la cantidad de carga que ha pasado a trav√©s de ellos, lo que les permite retener un estado resistivo espec√≠fico incluso despu√©s de que se ha eliminado la fuente de energ√≠a. Esta propiedad de memoria los hace especialmente adecuados para aplicaciones en almacenamiento de datos no vol√°til y procesamiento neurom√≥rfico.

La idea de los memristores fue propuesta por primera vez por Leon Chua en 1971. Chua predijo la existencia de un cuarto elemento pasivo fundamental en los circuitos el√©ctricos, junto con el resistor, el capacitor y el inductor. Este cuarto elemento, el memristor, tiene una relaci√≥n directa entre el flujo magn√©tico y la carga el√©ctrica, lo que le confiere sus propiedades √∫nicas. El memristor se define matem√°ticamente por la ecuaci√≥n \ref{eq:2} y \ref{eq:3}.

\begin{equation}
	V = R(\omega) \cdot I
	\label{eq:2}
\end{equation}

\begin{equation}
	\frac{d\omega}{dt} = f(I)
	\label{eq:3}
\end{equation}

Donde:
\begin{itemize}
	\item $V$ es la tensi√≥n a trav√©s del memristor.
	\item $I$ es la corriente que pasa a trav√©s del memristor.
	\item $R(\omega)$ es la resistencia dependiente del estado $\omega$, que es una funci√≥n de la carga hist√≥rica.
\end{itemize}

La capacidad de retener un estado resistivo espec√≠fico sin necesidad de energ√≠a continua.
Analog√≠a con las sinapsis: Su comportamiento es similar al de las sinapsis neuronales, donde la resistencia del memristor puede ajustarse de manera an√°loga a la fuerza sin√°ptica en las redes biol√≥gicas.
Densidad y eficiencia energ√©tica: Ofrecen alta densidad de almacenamiento y baja energ√≠a de operaci√≥n en comparaci√≥n con los dispositivos convencionales.

La integraci√≥n de memristores en redes neuronales artificiales ofrece una soluci√≥n prometedora para mejorar la eficiencia energ√©tica y la densidad de almacenamiento de estos sistemas. Los memristores pueden emular las sinapsis biol√≥gicas al proporcionar una forma no vol√°til y anal√≥gica de almacenamiento y procesamiento de informaci√≥n. En la siguiente secci√≥n, describimos nuestro enfoque para integrar un modelo de memristor de HP en una red neuronal artificial para la clasificaci√≥n del dataset MNIST, utilizando TensorFlow.

\section{Sistema propuesto}

El sistema propuesto integra un modelo de memristor de HP en una red neuronal artificial para la clasificaci√≥n del dataset MNIST utilizando la herramienta TensorFlow. Este enfoque busca aprovechar las propiedades √∫nicas de los memristores para mejorar la eficiencia energ√©tica y la capacidad de almacenamiento del sistema de aprendizaje autom√°tico. A continuaci√≥n, se detalla la arquitectura de la red neuronal, el funcionamiento del programa, el modelo de memristor utilizado y el algoritmo de entrenamiento.

\subsection{Arquitectura de la Red Neuronal}
La red neuronal utilizada en este trabajo es una red neuronal feedforward (FFNN) con las siguientes caracter√≠sticas:

Capa de Entrada: Una capa de entrada que toma im√°genes de 28x28 p√≠xeles del dataset MNIST, aplanando cada imagen en un vector de 784 dimensiones.
Capa Oculta Personalizada con Memristores: Una capa oculta personalizada que simula el comportamiento de un memristor de HP, con 128 unidades (neuronas).
Capa de Activaci√≥n: Una capa de activaci√≥n ReLU (Rectified Linear Unit) que introduce no linealidad en el modelo.
Capa de Salida: Una capa densa (fully connected) con 10 neuronas, cada una correspondiente a una de las 10 clases de d√≠gitos (0-9) del dataset MNIST.
Modelo de Memristor de HP
El modelo de memristor de HP se integra en la capa oculta personalizada. La actualizaci√≥n de los pesos sin√°pticos en esta capa se realiza mediante una ecuaci√≥n diferencial simplificada que describe la din√°mica de los memristores:

ùëÖ
(
ùë§
)
=
ùëÖ
ùëú
ùëõ
‚ãÖ
ùë§
+
ùëÖ
ùëú
ùëì
ùëì
‚ãÖ
(
1
‚àí
ùë§
)
R(w)=R 
on
‚Äã
‚ãÖw+R 
off
‚Äã
‚ãÖ(1‚àíw)
ùëë
ùë§
ùëë
ùë°
=
ùúá
‚ãÖ
ùêº
ùê∑
dt
dw
‚Äã
=Œº‚ãÖ 
D
I
‚Äã


Donde:

ùëÖ
ùëú
ùëõ
R 
on
‚Äã
y 
ùëÖ
ùëú
ùëì
ùëì
R 
off
‚Äã
son las resistencias m√≠nima y m√°xima del memristor.
ùë§
w es una variable de estado que representa la proporci√≥n de la capa de 
ùëá
ùëñ
ùëÇ
2
‚àí
ùë•
TiO 
2‚àíx
‚Äã
dopada.
ùúá
Œº es la movilidad de los vacantes de ox√≠geno.
ùê∑
D es el espesor de la pel√≠cula de 
ùëá
ùëñ
ùëÇ
2
TiO 
2
‚Äã
.
Funcionamiento del Programa
El programa se implementa en Python utilizando TensorFlow, una biblioteca de c√≥digo abierto para la construcci√≥n y entrenamiento de modelos de aprendizaje profundo. El flujo de trabajo del programa es el siguiente:

Carga y Preprocesamiento de Datos: Se carga el dataset MNIST y se normalizan las im√°genes dividiendo los valores de los p√≠xeles por 255.0 para que los valores est√©n en el rango [0, 1].
Definici√≥n de la Capa Personalizada: Se define una capa personalizada que simula el comportamiento del memristor de HP. Esta capa realiza la actualizaci√≥n de los pesos sin√°pticos en funci√≥n de la corriente que pasa a trav√©s de ellos.
Construcci√≥n del Modelo: Se construye el modelo de la red neuronal utilizando la capa personalizada junto con capas adicionales de activaci√≥n y salida.
Compilaci√≥n del Modelo: Se compila el modelo utilizando el optimizador Adam y la funci√≥n de p√©rdida de entrop√≠a cruzada esparsa para la clasificaci√≥n.
Entrenamiento del Modelo: Se entrena el modelo durante varias √©pocas en el conjunto de entrenamiento de MNIST.
Evaluaci√≥n del Modelo: Se eval√∫a el modelo en el conjunto de prueba de MNIST para determinar su precisi√≥n.
Predicci√≥n y Visualizaci√≥n: Se realizan predicciones en el conjunto de prueba y se visualizan algunas de las predicciones junto con sus etiquetas verdaderas.
Algoritmo de Entrenamiento
El algoritmo de entrenamiento utilizado es el gradiente descendente, implementado mediante el optimizador Adam. Este algoritmo ajusta los pesos sin√°pticos para minimizar la funci√≥n de p√©rdida. Los pasos del algoritmo son los siguientes:

Inicializaci√≥n de Pesos y Sesgos: Los pesos y sesgos de la red se inicializan aleatoriamente.
Propagaci√≥n hacia Adelante: Los datos de entrada se propagan a trav√©s de la red para generar predicciones.
C√°lculo de la P√©rdida: Se calcula la funci√≥n de p√©rdida que mide la discrepancia entre las predicciones y las etiquetas verdaderas.
Retropropagaci√≥n del Error: El error se propaga hacia atr√°s a trav√©s de la red para calcular los gradientes de la funci√≥n de p√©rdida respecto a los pesos y sesgos.
Actualizaci√≥n de Pesos y Sesgos: Los pesos y sesgos se actualizan utilizando los gradientes calculados y el algoritmo de optimizaci√≥n Adam.



\begin{table}[htbp]
\caption{Par√°metros $\frac{W}{L}$ del sistema}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{\textbf{Neurona}}&\multicolumn{2}{|c|}{\textbf{Sinapsis}} \\
\cline{1-4} 
\textbf{\textit{Par√°metro}} & \textbf{\textit{Valor}}& \textbf{\textit{Par√°metro}}& \textbf{\textit{Valor}} \\
\hline
$W_{1}$ & $0.2\mu m$ & $W_{1}$ & $5\mu m$\\
\hline
$W_{2}$ & $0.2\mu m$ & $W_{2}$ & $5\mu m$\\
\hline
$W_{3}$ & $1\mu m$ & $W_{3}$ & $1.5\mu m$\\
\hline
$W_{4}$ & $0.2\mu m$ & $W_{4}$ & $15\mu m$\\
\hline
$W_{5}$ & $0.2\mu m$ & $W_{5}$ & $1.5\mu m$\\
\hline
$W_{6}$ & $0.2\mu m$ & $W_{6}$ & $15\mu m$\\
\hline
$W_{7}$ & $0.2\mu m$ & & \\
\hline
$W_{8}$ & $0.2\mu m$ & & \\
\hline
$W_{9}$ & $1\mu m$ & & \\
\hline
\multicolumn{4}{l}{$^{*}$$L_{min}=0.1\mu m$ en ambos circuitos.}
\end{tabular}
\label{parametros}
\end{center}
\end{table}

\section*{Conclusi√≥n}

Este trabajo propone una novedosa integraci√≥n de memristores en redes neuronales artificiales para la clasificaci√≥n del dataset MNIST. Al utilizar un modelo de memristor de HP, demostramos c√≥mo estos dispositivos pueden mejorar la eficiencia energ√©tica y la capacidad de almacenamiento en sistemas de aprendizaje autom√°tico. Los resultados obtenidos muestran la viabilidad y el potencial de los memristores en aplicaciones de IA, abriendo el camino para futuros desarrollos en este campo emergente.

\bibliographystyle{IEEEtranN}
\bibliography{ref/referencias.bib}

\end{document}
